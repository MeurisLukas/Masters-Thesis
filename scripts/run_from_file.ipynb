{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f93b9a-0332-4ac2-86ec-72844274d650",
   "metadata": {},
   "source": [
    "# Master's thesis - Lukas Meuris - Run from file\n",
    "This script is used to run a 10 day forecast for the whole of 2020 using saved model parameters from a file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b2ceed-d1f6-4bab-be18-8b064ffb2676",
   "metadata": {},
   "source": [
    "# Installation and initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def77f71-6251-4ad3-b200-725b1f880473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import dataclasses\n",
    "import functools\n",
    "\n",
    "from google.cloud import storage\n",
    "from graphcast import autoregressive\n",
    "from graphcast import casting\n",
    "from graphcast import data_utils\n",
    "from graphcast import graphcast\n",
    "from graphcast import rollout\n",
    "from graphcast import normalization\n",
    "from graphcast import xarray_jax\n",
    "from graphcast import xarray_tree\n",
    "import haiku as hk\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import pandas as pd\n",
    "import xarray\n",
    "\n",
    "import optax\n",
    "\n",
    "import os\n",
    "import time\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2e190-7f56-4b79-806d-03ceabc95352",
   "metadata": {},
   "source": [
    "# Access data from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4634a1-2776-4b6d-9843-eae5a9bbda36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud Storage\n",
    "gcs_client = storage.Client.create_anonymous_client()\n",
    "gcs_bucket = gcs_client.get_bucket(\"dm_graphcast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e2fc8-53c6-431b-a5ab-d001b1f7e79b",
   "metadata": {},
   "source": [
    "## load model params from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4284205c-234f-4465-bd57-0fee1c655f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model params:\n",
    "\n",
    "def unflatten_dict(d, sep='//'):\n",
    "    result_dict = {}\n",
    "    for flat_key, value in d.items():\n",
    "        keys = flat_key.split(sep)\n",
    "        d = result_dict\n",
    "        for key in keys[:-1]:\n",
    "            if key not in d:\n",
    "                d[key] = {}\n",
    "            d = d[key]\n",
    "        d[keys[-1]] = value\n",
    "    return result_dict\n",
    "\n",
    "def load_model_params(file_path):\n",
    "    with np.load(file_path, allow_pickle=True) as npz_file:\n",
    "        # Convert NumPy arrays back to JAX arrays\n",
    "        jax_dict = {k: jnp.array(v) for k, v in npz_file.items()}\n",
    "    return unflatten_dict(jax_dict)\n",
    "\n",
    "\"\"\"\" change path below for correct params\"\"\"\n",
    "rel_path = 'models/params_64x32_mape.npz'\n",
    "params_path = os.path.join(os.path.dirname(os.getcwd()), rel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c808b-d462-4dba-80e7-1522e3eac7fc",
   "metadata": {},
   "source": [
    "## choose model parameters\n",
    "make sure these are the same as used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc8bb36d-3304-44b9-b87a-f1c3a2e7aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title choose model parameters\n",
    "random_mesh_size = 5 # mesh size: 4 - 6\n",
    "random_gnn_msg_steps = 8 # message passing steps: 1 - 32\n",
    "random_latent_size = 128 # latent size: 16,32,64,128,256,512\n",
    "random_levels = 13 # levels: 13 or 37\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b98c7d9-20a0-4166-bf40-9a0b0e455da2",
   "metadata": {},
   "source": [
    "## load model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619525b4-4bec-4af0-acb3-9fb47c4fe228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(resolution=0, mesh_size=5, latent_size=128, gnn_msg_steps=8, hidden_layers=1, radius_query_fraction_edge_length=0.6, mesh2grid_edge_normalization_factor=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title load the model parameters\n",
    "params = load_model_params(params_path)\n",
    "state = {}\n",
    "model_config = graphcast.ModelConfig(\n",
    "    resolution=0,\n",
    "    mesh_size=random_mesh_size,\n",
    "    latent_size=random_latent_size,\n",
    "    gnn_msg_steps=random_gnn_msg_steps,\n",
    "    hidden_layers=1,\n",
    "    radius_query_fraction_edge_length=0.6)\n",
    "task_config = graphcast.TaskConfig(\n",
    "    input_variables=graphcast.TASK.input_variables,\n",
    "    target_variables=graphcast.TASK.target_variables,\n",
    "    forcing_variables=graphcast.TASK.forcing_variables,\n",
    "    pressure_levels=graphcast.PRESSURE_LEVELS[random_levels],\n",
    "    input_duration=graphcast.TASK.input_duration,\n",
    ")\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffae8c5-890b-4e6c-82d9-e87a48f02821",
   "metadata": {},
   "source": [
    "## load obs_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa89cdc3-f5b0-41e1-b0b4-f2f77c075bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-lukas/Masters-Thesis/ERA5_data/obs_data.zarr\n"
     ]
    }
   ],
   "source": [
    "# load obs data:\n",
    "# Define the relative path to the file\n",
    "relative_path = \"ERA5_data/obs_data.zarr\"\n",
    "\n",
    "# Get the absolute path by joining the current directory with the relative path\n",
    "absolute_path = os.path.join(os.path.dirname(os.getcwd()), relative_path)\n",
    "print(absolute_path)\n",
    "\n",
    "# Open the Zarr file using xarray\n",
    "obs_data = xarray.open_zarr(absolute_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d82de-217f-4f4f-929a-fb22d9de719e",
   "metadata": {},
   "source": [
    "For evaluation we do a run with data from the year 2020.\n",
    "select the dates wanted from from the ERA5 dataset.\n",
    "\n",
    "important to note: even though we only want the to do a prediction for 2020, we select days well into 2021 to make sure the model doesn't run out of dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "765a8d2e-98e1-473d-be68-130975378e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get eval data from obs data:\n",
    "#time slice: 2020-01-01T00:00:00.000000000 to 2022-01-01T00:00:00.000000000 - EVALUATION\n",
    "eval_data = obs_data.sel(time=slice('2020-01-01T00:00:00.000000000','2021-03-31T00:00:00.000000000'))\n",
    "eval_steps = 40 # {1 - obs_data.sizes[\"time\"]-2} | 40 = 10days\n",
    "eval_data = eval_data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670532c2-efc3-41c2-a9f5-5ed339a2cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    eval_data, target_lead_times=slice(\"6h\", f\"{eval_steps*6}h\"),\n",
    "    **dataclasses.asdict(task_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ff618-d11e-4aa1-b172-33f631f52bd1",
   "metadata": {},
   "source": [
    "## load normalisation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d831f324-57c2-4f52-b02b-be308aa0ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gcs_bucket.blob(\"stats/diffs_stddev_by_level.nc\").open(\"rb\") as f:\n",
    "    diffs_stddev_by_level = xarray.load_dataset(f).compute()\n",
    "with gcs_bucket.blob(\"stats/mean_by_level.nc\").open(\"rb\") as f:\n",
    "    mean_by_level = xarray.load_dataset(f).compute()\n",
    "with gcs_bucket.blob(\"stats/stddev_by_level.nc\").open(\"rb\") as f:\n",
    "    stddev_by_level = xarray.load_dataset(f).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342e72e-a79a-4c59-b764-6323708c7d0a",
   "metadata": {},
   "source": [
    "## build functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1432d7-771e-4f93-930e-be4e0146b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build functions + jit:\n",
    "def construct_wrapped_graphcast(\n",
    "    model_config: graphcast.ModelConfig,\n",
    "    task_config: graphcast.TaskConfig):\n",
    "  \"\"\"Constructs and wraps the GraphCast Predictor.\"\"\"\n",
    "  # Deeper one-step predictor.\n",
    "  predictor = graphcast.GraphCast(model_config, task_config)\n",
    "\n",
    "  # Modify inputs/outputs to `graphcast.GraphCast` to handle conversion to\n",
    "  # from/to float32 to/from BFloat16.\n",
    "  predictor = casting.Bfloat16Cast(predictor)\n",
    "\n",
    "  # Modify inputs/outputs to `casting.Bfloat16Cast` so the casting to/from\n",
    "  # BFloat16 happens after applying normalization to the inputs/targets.\n",
    "  predictor = normalization.InputsAndResiduals(\n",
    "      predictor,\n",
    "      diffs_stddev_by_level=diffs_stddev_by_level,\n",
    "      mean_by_level=mean_by_level,\n",
    "      stddev_by_level=stddev_by_level)\n",
    "\n",
    "  # Wraps everything so the one-step model can produce trajectories.\n",
    "  predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)\n",
    "  return predictor\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, inputs, targets_template, forcings):\n",
    "  predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "  return predictor(inputs, targets_template=targets_template, forcings=forcings)\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def loss_fn(model_config, task_config, inputs, targets, forcings):\n",
    "  predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "  loss, diagnostics = predictor.loss(inputs, targets, forcings)\n",
    "  return xarray_tree.map_structure(\n",
    "      lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),\n",
    "      (loss, diagnostics))\n",
    "\n",
    "def grads_fn(params, state, inputs, targets, forcings, model_config, task_config):\n",
    "  def _aux(params, state, i, t, f):\n",
    "    (loss, diagnostics), next_state = loss_fn.apply(\n",
    "        params, state, jax.random.PRNGKey(0), model_config, task_config,\n",
    "        i, t, f)\n",
    "    return loss, (diagnostics, next_state)\n",
    "  (loss, (diagnostics, next_state)), grads = jax.value_and_grad(\n",
    "      _aux, has_aux=True)(params, state, inputs, targets, forcings)\n",
    "  return loss, diagnostics, next_state, grads\n",
    "\n",
    "# Jax doesn't seem to like passing configs as args through the jit. Passing it\n",
    "# in via partial (instead of capture by closure) forces jax to invalidate the\n",
    "# jit cache if you change configs.\n",
    "def with_configs(fn):\n",
    "  return functools.partial(\n",
    "      fn, model_config=model_config, task_config=task_config)\n",
    "\n",
    "# Always pass params and state, so the usage below are simpler\n",
    "def with_params(fn):\n",
    "  return functools.partial(fn, params=params, state=state)\n",
    "\n",
    "# Our models aren't stateful, so the state is always empty, so just return the\n",
    "# predictions. This is required by our rollout code, and generally simpler.\n",
    "def drop_state(fn):\n",
    "  return lambda **kw: fn(**kw)[0]\n",
    "\n",
    "init_jitted = jax.jit(with_configs(run_forward.init))\n",
    "\n",
    "if params is None:\n",
    "  params, state = init_jitted(\n",
    "      rng=jax.random.PRNGKey(0),\n",
    "      inputs=train_inputs.compute(),\n",
    "      targets_template=train_targets.compute(),\n",
    "      forcings=train_forcings.compute())\n",
    "\n",
    "loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply))))\n",
    "grads_fn_jitted = jax.jit(with_configs(grads_fn))\n",
    "run_forward_jitted = drop_state(with_params(jax.jit(with_configs(\n",
    "    run_forward.apply))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144e438-9f73-4c97-a695-d9bf44bdd365",
   "metadata": {},
   "source": [
    "## model running loop\n",
    "this makes 10day forecast for all dates selected.\n",
    "after each 10day forecast, the forecast is writing away unto a file to save on RAM memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73120d7a-ff66-4d7a-9355-0229adb7ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop:\n",
    "N = 1465 # = 366*4 + 1\n",
    "\"\"\"\" change path below for correct destination file\"\"\"\n",
    "store_path = 'predictions/pred_64x32_2020_mape.zarr'\n",
    "store_path = os.path.join(os.path.dirname(os.getcwd()), store_path)\n",
    "time_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "830798ae-cdfc-4722-a2b8-2a1ffdefef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping gradient checkpointing for sequence length of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2020-01-01 00:00:00\n",
      "time: 2020-01-08 00:00:00\n",
      "time: 2020-01-15 00:00:00\n",
      "time: 2020-01-22 00:00:00\n",
      "time: 2020-01-29 00:00:00\n",
      "time: 2020-02-05 00:00:00\n",
      "time: 2020-02-12 00:00:00\n",
      "time: 2020-02-19 00:00:00\n",
      "time: 2020-02-26 00:00:00\n",
      "time: 2020-03-04 00:00:00\n",
      "time: 2020-03-11 00:00:00\n",
      "time: 2020-03-18 00:00:00\n",
      "time: 2020-03-25 00:00:00\n",
      "time: 2020-04-01 00:00:00\n",
      "time: 2020-04-08 00:00:00\n",
      "time: 2020-04-15 00:00:00\n",
      "time: 2020-04-22 00:00:00\n",
      "time: 2020-04-29 00:00:00\n",
      "time: 2020-05-06 00:00:00\n",
      "time: 2020-05-13 00:00:00\n",
      "time: 2020-05-20 00:00:00\n",
      "time: 2020-05-27 00:00:00\n",
      "time: 2020-06-03 00:00:00\n",
      "time: 2020-06-10 00:00:00\n",
      "time: 2020-06-17 00:00:00\n",
      "time: 2020-06-24 00:00:00\n",
      "time: 2020-07-01 00:00:00\n",
      "time: 2020-07-08 00:00:00\n",
      "time: 2020-07-15 00:00:00\n",
      "time: 2020-07-22 00:00:00\n",
      "time: 2020-07-29 00:00:00\n",
      "time: 2020-08-05 00:00:00\n",
      "time: 2020-08-12 00:00:00\n",
      "time: 2020-08-19 00:00:00\n",
      "time: 2020-08-26 00:00:00\n",
      "time: 2020-09-02 00:00:00\n",
      "time: 2020-09-09 00:00:00\n",
      "time: 2020-09-16 00:00:00\n",
      "time: 2020-09-23 00:00:00\n",
      "time: 2020-09-30 00:00:00\n",
      "time: 2020-10-07 00:00:00\n",
      "time: 2020-10-14 00:00:00\n",
      "time: 2020-10-21 00:00:00\n",
      "time: 2020-10-28 00:00:00\n",
      "time: 2020-11-04 00:00:00\n",
      "time: 2020-11-11 00:00:00\n",
      "time: 2020-11-18 00:00:00\n",
      "time: 2020-11-25 00:00:00\n",
      "time: 2020-12-02 00:00:00\n",
      "time: 2020-12-09 00:00:00\n",
      "time: 2020-12-16 00:00:00\n",
      "time: 2020-12-23 00:00:00\n",
      "time: 2020-12-30 00:00:00\n",
      "prediction run completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(N):\n",
    "    eval_batch = eval_data.isel(time=slice(i, i + eval_steps + 2))\n",
    "    eval_batch = eval_batch.compute()\n",
    "\n",
    "    eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    eval_batch, target_lead_times=slice(\"6h\", f\"{eval_steps*6}h\"),\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "    time_value = pd.Timestamp(\"2020-01-01 00:00:00\") + pd.Timedelta(hours=i*6)\n",
    "    time_values.append(time_value)\n",
    "\n",
    "    prediction = rollout.chunked_prediction(\n",
    "        run_forward_jitted,\n",
    "        rng=jax.random.PRNGKey(0),\n",
    "        inputs=eval_inputs,\n",
    "        targets_template=eval_targets * np.nan,\n",
    "        forcings=eval_forcings)\n",
    "    \n",
    "    # modify dataset:\n",
    "    prediction = xarray.concat([eval_inputs.isel(time=1),prediction], dim='time') \n",
    "    prediction = prediction.rename({'time': 'prediction_timedelta'})\n",
    "    prediction = prediction.expand_dims(time=[time_value])\n",
    "    \n",
    "    # Write the prediction dataset to the zarr store\n",
    "    if i == 0:\n",
    "        prediction.to_zarr(store_path, mode='w',encoding={'time': {'dtype': 'float64', 'units': 'hours since 2020-01-01'}})\n",
    "    else:\n",
    "        prediction.to_zarr(store_path, append_dim='time')\n",
    "\n",
    "    if i% (4*7) == 0:\n",
    "        print(\"time:\" , time_value)\n",
    "\n",
    "print(\"prediction run completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c863d9-3bd4-491f-a5d6-2ddd88859203",
   "metadata": {},
   "source": [
    "## load prediction.\n",
    "load the prediction back in, to check if it is done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177cae8c-d152-4337-8a2b-94fa8d806f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path by joining the current directory with the relative path\n",
    "absolute_path = os.path.join(os.path.dirname(os.getcwd()), store_path)\n",
    "# Open the Zarr file using xarray\n",
    "pred_data = xarray.open_zarr(absolute_path)\n",
    "pred_data['2m_temperature'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c53a54c7-486f-45ce-9c59-547a79cf647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of pred_data dataset: 39.43942487239838 GB\n"
     ]
    }
   ],
   "source": [
    "# Check prediction file size.\n",
    "size_in_gb = pred_data.nbytes / (1024*1024*1024)\n",
    "print(\"Size of pred_data dataset:\", size_in_gb, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20bd68-e0c1-4841-b53f-af91d73209c1",
   "metadata": {},
   "source": [
    "Now the predictions run for 2020 is completed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MTLMenv",
   "language": "python",
   "name": "mtlmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
