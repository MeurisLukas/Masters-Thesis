{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4319b202d5a3853c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Master's thesis - Lukas Meuris - graphCast training - loss functions\n",
    "\n",
    "This notebook contains the code to train the graphCast model. \n",
    "We use this notebook to traint he graphcast model with different loss-functions to see the influence of it on the model performance.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602599318e266ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:32:29.980806Z",
     "start_time": "2024-04-25T15:29:54.800557Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title Pip install graphcast and dependencies\n",
    "\n",
    "#!pip install --upgrade https://github.com/deepmind/graphcast/archive/master.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c7bea05a9f82d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Installation and initialisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e247a864b8b52f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:33:27.376412Z",
     "start_time": "2024-04-25T15:33:24.026654Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import dataclasses\n",
    "import functools\n",
    "\n",
    "from google.cloud import storage\n",
    "from graphcast import autoregressive\n",
    "from graphcast import casting\n",
    "from graphcast import data_utils\n",
    "from graphcast import graphcast\n",
    "from graphcast import rollout\n",
    "from graphcast import normalization\n",
    "from graphcast import xarray_jax\n",
    "from graphcast import xarray_tree\n",
    "import haiku as hk\n",
    "import jax\n",
    "import numpy as np\n",
    "import xarray\n",
    "\n",
    "import os\n",
    "import time\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "def parse_file_parts(file_name):\n",
    "  return dict(part.split(\"-\", 1) for part in file_name.split(\"_\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53ac4e16811139b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Access data from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1a96b83614b53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:33:32.069605Z",
     "start_time": "2024-04-25T15:33:31.616191Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title Authenticate with Google Cloud Storage\n",
    "\n",
    "gcs_client = storage.Client.create_anonymous_client()\n",
    "gcs_bucket = gcs_client.get_bucket(\"dm_graphcast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6bbde098f5e90",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load the Data and initialize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6d61309fd5445",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Load the model parameters\n",
    "\n",
    "We use random parameters for the model initialization. \n",
    "We'll get random predictions, but we can change the model architecture.\n",
    "\n",
    "\n",
    "*Checkpoints vary across a few axes:*\n",
    "- *The mesh size specifies the internal graph representation of the earth. Smaller meshes will run faster but will have worse outputs. The mesh size does not affect the number of parameters of the model.*\n",
    "- *The resolution and number of pressure levels must match the data. Lower resolution and fewer levels will run a bit faster. Data resolution only affects the encoder/decoder.*\n",
    "- *All our models predict precipitation. However, ERA5 includes precipitation, while HRES does not. Our models marked as \"ERA5\" take precipitation as input and expect ERA5 data as input, while model marked \"ERA5-HRES\" do not take precipitation as input and are specifically trained to take HRES-fc0 as input (see the data section below).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174cfbbe28c88cd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:17.048201Z",
     "start_time": "2024-04-25T14:59:17.043Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title choose model parameters\n",
    "random_mesh_size = 5 # mesh size: 4 - 6\n",
    "random_gnn_msg_steps = 16 # message passing steps: 1 - 32\n",
    "random_latent_size = 512 # latent size: 16,32,64,128,256,512\n",
    "random_levels = 13 # levels: 13 or 37\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d651a7513e25b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:19.057161Z",
     "start_time": "2024-04-25T14:59:19.042071Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title load the model parameters\n",
    "params = None  # Filled in below\n",
    "state = {}\n",
    "model_config = graphcast.ModelConfig(\n",
    "    resolution=0,\n",
    "    mesh_size=random_mesh_size,\n",
    "    latent_size=random_latent_size,\n",
    "    gnn_msg_steps=random_gnn_msg_steps,\n",
    "    hidden_layers=1,\n",
    "    radius_query_fraction_edge_length=0.6)\n",
    "task_config = graphcast.TaskConfig(\n",
    "    input_variables=graphcast.TASK.input_variables,\n",
    "    target_variables=graphcast.TASK.target_variables,\n",
    "    forcing_variables=graphcast.TASK.forcing_variables,\n",
    "    pressure_levels=graphcast.PRESSURE_LEVELS[random_levels],\n",
    "    input_duration=graphcast.TASK.input_duration,\n",
    ")\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1137cdf0973c0b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load the ERA5 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b555036ff6ba79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:24.207615Z",
     "start_time": "2024-04-25T14:59:23.822646Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title Get and filter the list of available example datasets, then select one.\n",
    "\n",
    "# Define the relative path to the file\n",
    "relative_path = \"ERA5_data/obs_data.zarr\"\n",
    "\n",
    "# Get the absolute path by joining the current directory with the relative path\n",
    "absolute_path = os.path.join(os.path.dirname(os.getcwd()), relative_path)\n",
    "print(absolute_path)\n",
    "\n",
    "# Open the Zarr file using xarray\n",
    "obs_data = xarray.open_zarr(absolute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f280a9e-b305-4cbf-80c6-a7553c4e4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view obs_data:\n",
    "obs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f0775-6412-4315-a58c-562fd0e6ede8",
   "metadata": {},
   "source": [
    "## extract the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5863be6de50f40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:29.611908Z",
     "start_time": "2024-04-25T14:59:29.588587Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#time slice: 1980-01-01T00:00:00.000000000 to 2019-12-31T00:00:00.000000000 - TRAINING\n",
    "train_data = obs_data.sel(time=slice('1980-01-01T00:00:00.000000000','2019-12-31T00:00:00.000000000'))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6bf29-0bfb-4d60-bfd0-b6f512ea61e1",
   "metadata": {},
   "source": [
    "## extract the evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78cd01b90b854dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:31.673446Z",
     "start_time": "2024-04-25T14:59:31.645146Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#time slice: 2020-01-01T00:00:00.000000000 to 2022-01-01T00:00:00.000000000 - EVALUATION\n",
    "eval_data = obs_data.sel(time=slice('2020-01-01T00:00:00.000000000','2021-01-01T00:00:00.000000000'))\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad68aa3f-1d19-4a1f-aac9-7825c47de7f7",
   "metadata": {},
   "source": [
    "## choose number of training and evaluation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424741e3f3b393b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:27.452311Z",
     "start_time": "2024-04-25T14:59:27.448182Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title Choose training and eval data to extract\n",
    "train_steps = 12  # {1 - obs_data.sizes[\"time\"]-2} | 12 = 3days\n",
    "eval_steps = 40 # {1 - obs_data.sizes[\"time\"]-2} | 40 = 10days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d0a1b-b4af-4b81-a78a-91aa8b96501f",
   "metadata": {},
   "source": [
    "## extract training and eval inputs, targets and forcings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1e62db03aaeaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:33.739762Z",
     "start_time": "2024-04-25T14:59:33.708680Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    train_data, target_lead_times=slice(\"6h\", f\"{train_steps*6}h\"),\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    eval_data, target_lead_times=slice(\"6h\", f\"{eval_steps*6}h\"),\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "print(\"All data:  \", obs_data.dims.mapping)\n",
    "print(\"Train Inputs:  \", train_inputs.dims.mapping)\n",
    "print(\"Train Targets: \", train_targets.dims.mapping)\n",
    "print(\"Train Forcings:\", train_forcings.dims.mapping)\n",
    "print(\"Eval Inputs:   \", eval_inputs.dims.mapping)\n",
    "print(\"Eval Targets:  \", eval_targets.dims.mapping)\n",
    "print(\"Eval Forcings: \", eval_forcings.dims.mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5416fc-18d0-4c3f-b6b8-a95b95127a19",
   "metadata": {},
   "source": [
    "## Load normalization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ab3947cd265af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:38.210326Z",
     "start_time": "2024-04-25T14:59:36.595899Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with gcs_bucket.blob(\"stats/diffs_stddev_by_level.nc\").open(\"rb\") as f:\n",
    "    diffs_stddev_by_level = xarray.load_dataset(f).compute()\n",
    "with gcs_bucket.blob(\"stats/mean_by_level.nc\").open(\"rb\") as f:\n",
    "    mean_by_level = xarray.load_dataset(f).compute()\n",
    "with gcs_bucket.blob(\"stats/stddev_by_level.nc\").open(\"rb\") as f:\n",
    "    stddev_by_level = xarray.load_dataset(f).compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f95ba9-4657-4313-a090-2038c32b825a",
   "metadata": {},
   "source": [
    "## Build jitted functions, and possibly initialize random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fc1c864187c8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:41.930757Z",
     "start_time": "2024-04-25T14:59:41.582129Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def construct_wrapped_graphcast(\n",
    "    model_config: graphcast.ModelConfig,\n",
    "    task_config: graphcast.TaskConfig):\n",
    "  \"\"\"Constructs and wraps the GraphCast Predictor.\"\"\"\n",
    "  # Deeper one-step predictor.\n",
    "  predictor = graphcast.GraphCast(model_config, task_config)\n",
    "\n",
    "  # Modify inputs/outputs to `graphcast.GraphCast` to handle conversion to\n",
    "  # from/to float32 to/from BFloat16.\n",
    "  predictor = casting.Bfloat16Cast(predictor)\n",
    "\n",
    "  # Modify inputs/outputs to `casting.Bfloat16Cast` so the casting to/from\n",
    "  # BFloat16 happens after applying normalization to the inputs/targets.\n",
    "  predictor = normalization.InputsAndResiduals(\n",
    "      predictor,\n",
    "      diffs_stddev_by_level=diffs_stddev_by_level,\n",
    "      mean_by_level=mean_by_level,\n",
    "      stddev_by_level=stddev_by_level)\n",
    "\n",
    "  # Wraps everything so the one-step model can produce trajectories.\n",
    "  predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)\n",
    "  return predictor\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, inputs, targets_template, forcings):\n",
    "  predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "  return predictor(inputs, targets_template=targets_template, forcings=forcings)\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def loss_fn(model_config, task_config, inputs, targets, forcings):\n",
    "  predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "  loss, diagnostics = predictor.loss(inputs, targets, forcings)\n",
    "  return xarray_tree.map_structure(\n",
    "      lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),\n",
    "      (loss, diagnostics))\n",
    "\n",
    "def grads_fn(params, state, model_config, task_config, inputs, targets, forcings):\n",
    "  def _aux(params, state, i, t, f):\n",
    "    (loss, diagnostics), next_state = loss_fn.apply(\n",
    "        params, state, jax.random.PRNGKey(0), model_config, task_config,\n",
    "        i, t, f)\n",
    "    return loss, (diagnostics, next_state)\n",
    "  (loss, (diagnostics, next_state)), grads = jax.value_and_grad(\n",
    "      _aux, has_aux=True)(params, state, inputs, targets, forcings)\n",
    "  return loss, diagnostics, next_state, grads\n",
    "\n",
    "# Jax doesn't seem to like passing configs as args through the jit. Passing it\n",
    "# in via partial (instead of capture by closure) forces jax to invalidate the\n",
    "# jit cache if you change configs.\n",
    "def with_configs(fn):\n",
    "  return functools.partial(\n",
    "      fn, model_config=model_config, task_config=task_config)\n",
    "\n",
    "# Always pass params and state, so the usage below are simpler\n",
    "def with_params(fn):\n",
    "  return functools.partial(fn, params=params, state=state)\n",
    "\n",
    "# Our models aren't stateful, so the state is always empty, so just return the\n",
    "# predictions. This is required by our rollout code, and generally simpler.\n",
    "def drop_state(fn):\n",
    "  return lambda **kw: fn(**kw)[0]\n",
    "\n",
    "init_jitted = jax.jit(with_configs(run_forward.init))\n",
    "\n",
    "if params is None:\n",
    "  params, state = init_jitted(\n",
    "      rng=jax.random.PRNGKey(0),\n",
    "      inputs=train_inputs.compute(),\n",
    "      targets_template=train_targets.compute(),\n",
    "      forcings=train_forcings.compute())\n",
    "\n",
    "loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply))))\n",
    "grads_fn_jitted = with_params(jax.jit(with_configs(grads_fn)))\n",
    "run_forward_jitted = drop_state(with_params(jax.jit(with_configs(\n",
    "    run_forward.apply))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc4a6bd35432c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:45.292410Z",
     "start_time": "2024-04-25T14:59:45.265578Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title config check\n",
    "\n",
    "assert model_config.resolution in (0, 360. / eval_inputs.sizes[\"lon\"]), (\n",
    "  \"Model resolution doesn't match the data resolution. You likely want to \"\n",
    "  \"re-filter the dataset list, and download the correct data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf179572c8eea101",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Train the model\n",
    "\n",
    "The following operations require a large amount of memory and, depending on the accelerator being used, will only fit the very small \"random\" model on low resolution data. It uses the number of training steps selected above.\n",
    "\n",
    "The first time executing the cell takes more time, as it include the time to jit the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7ab35ea26c8ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T13:47:03.576728Z",
     "start_time": "2024-04-11T13:45:39.472721Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# @title Loss computation (autoregressive loss over multiple steps)\n",
    "loss, diagnostics = loss_fn_jitted(\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=train_inputs,\n",
    "    targets=train_targets,\n",
    "    forcings=train_forcings)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57fd604-ac2a-4b58-8491-b44918453bf5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title Gradient computation (backprop through time)\n",
    "\n",
    "loss, diagnostics, next_state, grads = grads_fn_jitted(\n",
    "    inputs=train_inputs,\n",
    "    targets=train_targets,\n",
    "    forcings=train_forcings)\n",
    "mean_grad = np.mean(jax.tree_util.tree_flatten(jax.tree_util.tree_map(lambda x: np.abs(x).mean(), grads))[0])\n",
    "print(f\"Loss: {loss:.4f}, Mean |grad|: {mean_grad:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb2774-d8bb-460d-a798-6e8f9d87b580",
   "metadata": {},
   "source": [
    "# Model training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a76184-296c-4bfc-bcf3-7977ec38cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = train_data.sizes['time'] - train_steps - 4\n",
    "loss_array = []\n",
    "\n",
    "for i in range(N):\n",
    "    train_batch = train_data.isel(time=slice(i, i + train_steps + 2))\n",
    "    train_batch = train_batch.compute()\n",
    "\n",
    "    train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    train_batch, target_lead_times=slice(\"6h\", f\"{train_steps*6}h\"),\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "    loss, diagnostics = loss_fn_jitted(\n",
    "        rng=jax.random.PRNGKey(0),\n",
    "        inputs=train_inputs,\n",
    "        targets=train_targets,\n",
    "        forcings=train_forcings)\n",
    "    loss_array.append(loss)\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print(\"I:\", i , \" - Loss:\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334b1529acc30f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34d5dea497941b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T13:48:28.913640Z",
     "start_time": "2024-04-11T13:47:03.595074Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title Autoregressive rollout (loop in python)\n",
    "\n",
    "print(\"Inputs:  \", eval_inputs.dims.mapping)\n",
    "print(\"Targets: \", eval_targets.dims.mapping)\n",
    "print(\"Forcings:\", eval_forcings.dims.mapping)\n",
    "\n",
    "predictions = rollout.chunked_prediction(\n",
    "    run_forward_jitted,\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=eval_inputs,\n",
    "    targets_template=eval_targets * np.nan,\n",
    "    forcings=eval_forcings)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f4fb3-49e6-4a15-aa1d-18bb0675c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of the array:\n",
    "print(\"size of array:\", (predictions.nbytes/ (1024*1024)) ,\"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c556a-29a6-4712-b253-00c12b586ffc",
   "metadata": {},
   "source": [
    "# Model running loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a8b37-2d4a-4ce4-982b-4dcd3b768b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = eval_data.sizes['time'] - eval_steps - 4\n",
    "# Open a zarr store to write predictions to disk\n",
    "store = xr.backends.ZarrStore(\"../preds/predictions.zarr\", \"w\")\n",
    "\n",
    "for i in range(10):\n",
    "    eval_batch = eval_data.isel(time=slice(i, i + eval_steps + 2))\n",
    "    eval_batch = eval_batch.compute()\n",
    "\n",
    "    eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    eval_batch, target_lead_times=slice(\"6h\", f\"{eval_steps*6}h\"),\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "    predictions = rollout.chunked_prediction(\n",
    "        run_forward_jitted,\n",
    "        rng=jax.random.PRNGKey(0),\n",
    "        inputs=eval_inputs,\n",
    "        targets_template=eval_targets * np.nan,\n",
    "        forcings=eval_forcings)\n",
    "    # Write the prediction dataset to the zarr store\n",
    "    predictions.to_zarr(store, group=f\"prediction_time_step_{prediction_time_step}\", mode=\"a\")\n",
    "\n",
    "# Close the zarr store\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0872f8-a28f-4afb-b355-5a2a77f744b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the predictions back into memory and concatenate them along the 'pred_times' dimension\n",
    "predictions = xr.open_zarr(\"../preds/predictions.zarr\", concat_dim=\"prediction_time_step\")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481c19e5f3d2785",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Save the evaluation run to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a137abf8ea1ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the Zarr file\n",
    "zarr_path = \"Eval_preds/mse_64x32_2020.zarr\"\n",
    "\n",
    "# Save the dataset to the Zarr file\n",
    "predictions.to_zarr(zarr_path, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a73a19c6364468",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Save the model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86022304a4444f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T13:48:29.022317Z",
     "start_time": "2024-04-11T13:48:28.919472Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title saving the model\n",
    "# Save the model\n",
    "np.savez(\"models/model_64x32_mse.npz\", **hk.data_structures.to_mutable_dict(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c5fb7cab06b29",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now, our trained model is saved to a file ,which can be used to load and run again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MTLMenv",
   "language": "python",
   "name": "mtlmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
