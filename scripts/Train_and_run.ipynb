{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4319b202d5a3853c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Master's thesis - Lukas Meuris - graphCast training - loss functions\n",
    "\n",
    "This notebook contains the code to train the graphCast model. \n",
    "We use this notebook to traint he graphcast model with different loss-functions to see the influence of it on the model performance.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602599318e266ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:32:29.980806Z",
     "start_time": "2024-04-25T15:29:54.800557Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title Pip install graphcast and dependencies\n",
    "\n",
    "#!pip install --upgrade https://github.com/deepmind/graphcast/archive/master.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c7bea05a9f82d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Installation and initialisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e247a864b8b52f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:33:27.376412Z",
     "start_time": "2024-04-25T15:33:24.026654Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import dataclasses\n",
    "import functools\n",
    "\n",
    "from google.cloud import storage\n",
    "from graphcast import autoregressive\n",
    "from graphcast import casting\n",
    "from graphcast import data_utils\n",
    "from graphcast import graphcast\n",
    "from graphcast import rollout\n",
    "from graphcast import normalization\n",
    "from graphcast import xarray_jax\n",
    "from graphcast import xarray_tree\n",
    "import haiku as hk\n",
    "import jax\n",
    "import numpy as np\n",
    "import xarray\n",
    "\n",
    "import optax\n",
    "\n",
    "import os\n",
    "import time\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53ac4e16811139b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Access data from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f1a96b83614b53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:33:32.069605Z",
     "start_time": "2024-04-25T15:33:31.616191Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title Authenticate with Google Cloud Storage\n",
    "\n",
    "gcs_client = storage.Client.create_anonymous_client()\n",
    "gcs_bucket = gcs_client.get_bucket(\"dm_graphcast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6bbde098f5e90",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load the Data and initialize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6d61309fd5445",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Load the model parameters\n",
    "\n",
    "We use random parameters for the model initialization. \n",
    "We'll get random predictions, but we can change the model architecture.\n",
    "\n",
    "\n",
    "*Checkpoints vary across a few axes:*\n",
    "- *The mesh size specifies the internal graph representation of the earth. Smaller meshes will run faster but will have worse outputs. The mesh size does not affect the number of parameters of the model.*\n",
    "- *The resolution and number of pressure levels must match the data. Lower resolution and fewer levels will run a bit faster. Data resolution only affects the encoder/decoder.*\n",
    "- *All our models predict precipitation. However, ERA5 includes precipitation, while HRES does not. Our models marked as \"ERA5\" take precipitation as input and expect ERA5 data as input, while model marked \"ERA5-HRES\" do not take precipitation as input and are specifically trained to take HRES-fc0 as input (see the data section below).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174cfbbe28c88cd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:17.048201Z",
     "start_time": "2024-04-25T14:59:17.043Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title choose model parameters\n",
    "random_mesh_size = 5 # mesh size: 4 - 6\n",
    "random_gnn_msg_steps = 8 # message passing steps: 1 - 32\n",
    "random_latent_size = 128 # latent size: 16,32,64,128,256,512\n",
    "random_levels = 13 # levels: 13 or 37\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f4d651a7513e25b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:19.057161Z",
     "start_time": "2024-04-25T14:59:19.042071Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(resolution=0, mesh_size=5, latent_size=128, gnn_msg_steps=8, hidden_layers=1, radius_query_fraction_edge_length=0.6, mesh2grid_edge_normalization_factor=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title load the model parameters\n",
    "params = None  # Filled in below\n",
    "state = {}\n",
    "model_config = graphcast.ModelConfig(\n",
    "    resolution=0,\n",
    "    mesh_size=random_mesh_size,\n",
    "    latent_size=random_latent_size,\n",
    "    gnn_msg_steps=random_gnn_msg_steps,\n",
    "    hidden_layers=1,\n",
    "    radius_query_fraction_edge_length=0.6)\n",
    "task_config = graphcast.TaskConfig(\n",
    "    input_variables=graphcast.TASK.input_variables,\n",
    "    target_variables=graphcast.TASK.target_variables,\n",
    "    forcing_variables=graphcast.TASK.forcing_variables,\n",
    "    pressure_levels=graphcast.PRESSURE_LEVELS[random_levels],\n",
    "    input_duration=graphcast.TASK.input_duration,\n",
    ")\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1137cdf0973c0b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Load the ERA5 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b555036ff6ba79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:24.207615Z",
     "start_time": "2024-04-25T14:59:23.822646Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-lukas/Masters-Thesis/ERA5_data/obs_data.zarr\n"
     ]
    }
   ],
   "source": [
    "# @title Get and filter the list of available example datasets, then select one.\n",
    "\n",
    "# Define the relative path to the file\n",
    "relative_path = \"ERA5_data/obs_data.zarr\"\n",
    "\n",
    "# Get the absolute path by joining the current directory with the relative path\n",
    "absolute_path = os.path.join(os.path.dirname(os.getcwd()), relative_path)\n",
    "print(absolute_path)\n",
    "\n",
    "# Open the Zarr file using xarray\n",
    "obs_data = xarray.open_zarr(absolute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f280a9e-b305-4cbf-80c6-a7553c4e4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view obs_data:\n",
    "obs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f0775-6412-4315-a58c-562fd0e6ede8",
   "metadata": {},
   "source": [
    "## extract the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba5863be6de50f40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:29.611908Z",
     "start_time": "2024-04-25T14:59:29.588587Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#time slice: 1980-01-01T00:00:00.000000000 to 2019-12-31T00:00:00.000000000 - TRAINING\n",
    "train_data = obs_data.sel(time=slice('1980-01-01T00:00:00.000000000','2019-12-31T00:00:00.000000000'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6bf29-0bfb-4d60-bfd0-b6f512ea61e1",
   "metadata": {},
   "source": [
    "## extract the evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f78cd01b90b854dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:31.673446Z",
     "start_time": "2024-04-25T14:59:31.645146Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#time slice: 2020-01-01T00:00:00.000000000 to 2022-01-01T00:00:00.000000000 - EVALUATION\n",
    "eval_data = obs_data.sel(time=slice('2020-01-01T00:00:00.000000000','2021-01-01T00:00:00.000000000'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad68aa3f-1d19-4a1f-aac9-7825c47de7f7",
   "metadata": {},
   "source": [
    "## choose number of training and evaluation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424741e3f3b393b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:27.452311Z",
     "start_time": "2024-04-25T14:59:27.448182Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title Choose training and eval data to extract\n",
    "train_steps = 12  # {1 - obs_data.sizes[\"time\"]-2} | 12 = 3days\n",
    "eval_steps = 40 # {1 - obs_data.sizes[\"time\"]-2} | 40 = 10days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d0a1b-b4af-4b81-a78a-91aa8b96501f",
   "metadata": {},
   "source": [
    "## extract training and eval inputs, targets and forcings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64d1e62db03aaeaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:33.739762Z",
     "start_time": "2024-04-25T14:59:33.708680Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    train_data, target_lead_times=slice(\"6h\", f\"{train_steps*6}h\"),\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    eval_data, target_lead_times=slice(\"6h\", f\"{eval_steps*6}h\"),\n",
    "    **dataclasses.asdict(task_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5416fc-18d0-4c3f-b6b8-a95b95127a19",
   "metadata": {},
   "source": [
    "## Load normalization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "193ab3947cd265af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:38.210326Z",
     "start_time": "2024-04-25T14:59:36.595899Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with gcs_bucket.blob(\"stats/diffs_stddev_by_level.nc\").open(\"rb\") as f:\n",
    "    diffs_stddev_by_level = xarray.load_dataset(f).compute()\n",
    "with gcs_bucket.blob(\"stats/mean_by_level.nc\").open(\"rb\") as f:\n",
    "    mean_by_level = xarray.load_dataset(f).compute()\n",
    "with gcs_bucket.blob(\"stats/stddev_by_level.nc\").open(\"rb\") as f:\n",
    "    stddev_by_level = xarray.load_dataset(f).compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f95ba9-4657-4313-a090-2038c32b825a",
   "metadata": {},
   "source": [
    "## Build jitted functions, and possibly initialize random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e60fc1c864187c8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T14:59:41.930757Z",
     "start_time": "2024-04-25T14:59:41.582129Z"
    },
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def construct_wrapped_graphcast(\n",
    "    model_config: graphcast.ModelConfig,\n",
    "    task_config: graphcast.TaskConfig):\n",
    "  \"\"\"Constructs and wraps the GraphCast Predictor.\"\"\"\n",
    "  # Deeper one-step predictor.\n",
    "  predictor = graphcast.GraphCast(model_config, task_config)\n",
    "\n",
    "  # Modify inputs/outputs to `graphcast.GraphCast` to handle conversion to\n",
    "  # from/to float32 to/from BFloat16.\n",
    "  predictor = casting.Bfloat16Cast(predictor)\n",
    "\n",
    "  # Modify inputs/outputs to `casting.Bfloat16Cast` so the casting to/from\n",
    "  # BFloat16 happens after applying normalization to the inputs/targets.\n",
    "  predictor = normalization.InputsAndResiduals(\n",
    "      predictor,\n",
    "      diffs_stddev_by_level=diffs_stddev_by_level,\n",
    "      mean_by_level=mean_by_level,\n",
    "      stddev_by_level=stddev_by_level)\n",
    "\n",
    "  # Wraps everything so the one-step model can produce trajectories.\n",
    "  predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)\n",
    "  return predictor\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, inputs, targets_template, forcings):\n",
    "  predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "  return predictor(inputs, targets_template=targets_template, forcings=forcings)\n",
    "\n",
    "\n",
    "@hk.transform_with_state\n",
    "def loss_fn(model_config, task_config, inputs, targets, forcings):\n",
    "  predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "  loss, diagnostics = predictor.loss(inputs, targets, forcings)\n",
    "  return xarray_tree.map_structure(\n",
    "      lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),\n",
    "      (loss, diagnostics))\n",
    "\n",
    "def grads_fn(params, state, inputs, targets, forcings, model_config, task_config):\n",
    "  def _aux(params, state, i, t, f):\n",
    "    (loss, diagnostics), next_state = loss_fn.apply(\n",
    "        params, state, jax.random.PRNGKey(0), model_config, task_config,\n",
    "        i, t, f)\n",
    "    return loss, (diagnostics, next_state)\n",
    "  (loss, (diagnostics, next_state)), grads = jax.value_and_grad(\n",
    "      _aux, has_aux=True)(params, state, inputs, targets, forcings)\n",
    "  return loss, diagnostics, next_state, grads\n",
    "\n",
    "# Jax doesn't seem to like passing configs as args through the jit. Passing it\n",
    "# in via partial (instead of capture by closure) forces jax to invalidate the\n",
    "# jit cache if you change configs.\n",
    "def with_configs(fn):\n",
    "  return functools.partial(\n",
    "      fn, model_config=model_config, task_config=task_config)\n",
    "\n",
    "# Always pass params and state, so the usage below are simpler\n",
    "def with_params(fn):\n",
    "  return functools.partial(fn, params=params, state=state)\n",
    "\n",
    "# Our models aren't stateful, so the state is always empty, so just return the\n",
    "# predictions. This is required by our rollout code, and generally simpler.\n",
    "def drop_state(fn):\n",
    "  return lambda **kw: fn(**kw)[0]\n",
    "\n",
    "init_jitted = jax.jit(with_configs(run_forward.init))\n",
    "\n",
    "if params is None:\n",
    "  params, state = init_jitted(\n",
    "      rng=jax.random.PRNGKey(0),\n",
    "      inputs=train_inputs.compute(),\n",
    "      targets_template=train_targets.compute(),\n",
    "      forcings=train_forcings.compute())\n",
    "\n",
    "loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply))))\n",
    "grads_fn_jitted = jax.jit(with_configs(grads_fn))\n",
    "run_forward_jitted = drop_state(with_params(jax.jit(with_configs(\n",
    "    run_forward.apply))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae9fcb1d-9b91-43f4-9b4c-0309bcb1e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimiser\n",
    "lr = 1e-3\n",
    "optimiser = optax.adam(lr, b1=0.9, b2=0.999, eps=1e-8)\n",
    "opt_state = optimiser.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb2774-d8bb-460d-a798-6e8f9d87b580",
   "metadata": {},
   "source": [
    "# Model training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a76184-296c-4bfc-bcf3-7977ec38cebf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 0  - Loss: 29.640625\n",
      "I: 100  - Loss: 7.328125\n",
      "I: 200  - Loss: 6.7109375\n",
      "I: 300  - Loss: 7.670573\n",
      "I: 400  - Loss: 6.6367188\n",
      "I: 500  - Loss: 6.404948\n",
      "I: 600  - Loss: 6.651042\n",
      "I: 700  - Loss: 6.5585938\n",
      "I: 800  - Loss: 6.059896\n",
      "I: 900  - Loss: 6.2981772\n",
      "I: 1000  - Loss: 6.373698\n",
      "I: 1100  - Loss: 6.7382812\n",
      "I: 1200  - Loss: 6.4440107\n",
      "I: 1300  - Loss: 6.359375\n",
      "I: 1400  - Loss: 6.7057295\n",
      "I: 1500  - Loss: 6.3919272\n",
      "I: 1600  - Loss: 6.5898438\n",
      "I: 1700  - Loss: 6.3138022\n",
      "I: 1800  - Loss: 6.1054688\n",
      "I: 1900  - Loss: 6.2148438\n",
      "I: 2000  - Loss: 6.4570312\n",
      "I: 2100  - Loss: 5.828125\n",
      "I: 2200  - Loss: 6.186198\n",
      "I: 2300  - Loss: 5.9179688\n",
      "I: 2400  - Loss: 6.1484375\n",
      "I: 2500  - Loss: 5.981771\n",
      "I: 2600  - Loss: 5.8203125\n",
      "I: 2700  - Loss: 6.3242188\n",
      "I: 2800  - Loss: 6.4804688\n",
      "I: 2900  - Loss: 6.0403647\n",
      "I: 3000  - Loss: 6.6679688\n",
      "I: 3100  - Loss: 6.545573\n",
      "I: 3200  - Loss: 5.916667\n",
      "I: 3300  - Loss: 6.4101562\n",
      "I: 3400  - Loss: 6.4257812\n",
      "I: 3500  - Loss: 5.6927085\n",
      "I: 3600  - Loss: 5.6640625\n",
      "I: 3700  - Loss: 6.260417\n",
      "I: 3800  - Loss: 5.921875\n",
      "I: 3900  - Loss: 5.9309897\n",
      "I: 4000  - Loss: 6.041667\n",
      "I: 4100  - Loss: 6.466146\n",
      "I: 4200  - Loss: 6.171875\n",
      "I: 4300  - Loss: 6.6731772\n",
      "I: 4400  - Loss: 6.3841147\n",
      "I: 4500  - Loss: 6.1002607\n",
      "I: 4600  - Loss: 6.2747397\n",
      "I: 4700  - Loss: 6.078125\n",
      "I: 4800  - Loss: 6.515625\n",
      "I: 4900  - Loss: 5.7591147\n",
      "I: 5000  - Loss: 5.8216147\n",
      "I: 5100  - Loss: 5.8307295\n",
      "I: 5200  - Loss: 5.795573\n",
      "I: 5300  - Loss: 5.734375\n",
      "I: 5400  - Loss: 5.791667\n",
      "I: 5500  - Loss: 5.8346357\n",
      "I: 5600  - Loss: 6.046875\n",
      "I: 5700  - Loss: 6.2369795\n",
      "I: 5800  - Loss: 6.7226562\n",
      "I: 5900  - Loss: 6.1653647\n",
      "I: 6000  - Loss: 6.2708335\n",
      "I: 6100  - Loss: 5.9427085\n",
      "I: 6200  - Loss: 5.966146\n",
      "I: 6300  - Loss: 6.5664062\n",
      "I: 6400  - Loss: 6.0963545\n",
      "I: 6500  - Loss: 5.572917\n",
      "I: 6600  - Loss: 5.984375\n",
      "I: 6700  - Loss: 5.8932295\n",
      "I: 6800  - Loss: 6.028646\n",
      "I: 6900  - Loss: 6.1302085\n",
      "I: 7000  - Loss: 6.557292\n",
      "I: 7100  - Loss: 5.9622397\n",
      "I: 7200  - Loss: 6.3502607\n",
      "I: 7300  - Loss: 6.1966147\n",
      "I: 7400  - Loss: 5.8164062\n",
      "I: 7500  - Loss: 6.0026045\n",
      "I: 7600  - Loss: 5.7695312\n",
      "I: 7700  - Loss: 6.4231772\n",
      "I: 7800  - Loss: 5.856771\n",
      "I: 7900  - Loss: 5.936198\n",
      "I: 8000  - Loss: 6.3177085\n",
      "I: 8100  - Loss: 5.6471357\n",
      "I: 8200  - Loss: 5.4544272\n",
      "I: 8300  - Loss: 6.2773438\n",
      "I: 8400  - Loss: 6.1002607\n",
      "I: 8500  - Loss: 5.682292\n",
      "I: 8600  - Loss: 5.9244795\n",
      "I: 8700  - Loss: 6.2682295\n",
      "I: 8800  - Loss: 5.9088545\n",
      "I: 8900  - Loss: 6.076823\n",
      "I: 9000  - Loss: 6.03125\n",
      "I: 9100  - Loss: 6.28125\n",
      "I: 9200  - Loss: 6.1145835\n",
      "I: 9300  - Loss: 5.947917\n",
      "I: 9400  - Loss: 6.2825522\n",
      "I: 9500  - Loss: 6.139323\n",
      "I: 9600  - Loss: 5.796875\n",
      "I: 9700  - Loss: 5.731771\n",
      "I: 9800  - Loss: 5.776042\n",
      "I: 9900  - Loss: 6.1432295\n",
      "I: 10000  - Loss: 6.3684897\n",
      "I: 10100  - Loss: 5.90625\n",
      "I: 10200  - Loss: 5.9140625\n",
      "I: 10300  - Loss: 6.2773438\n",
      "I: 10400  - Loss: 5.8776045\n",
      "I: 10500  - Loss: 6.0182295\n",
      "I: 10600  - Loss: 6.1783857\n",
      "I: 10700  - Loss: 5.8059897\n",
      "I: 10800  - Loss: 5.8932295\n",
      "I: 10900  - Loss: 6.0520835\n",
      "I: 11000  - Loss: 5.7773438\n",
      "I: 11100  - Loss: 5.717448\n",
      "I: 11200  - Loss: 5.701823\n",
      "I: 11300  - Loss: 6.076823\n",
      "I: 11400  - Loss: 6.1601562\n",
      "I: 11500  - Loss: 6.231771\n",
      "I: 11600  - Loss: 6.296875\n",
      "I: 11700  - Loss: 6.0585938\n",
      "I: 11800  - Loss: 6.2851562\n",
      "I: 11900  - Loss: 6.5664062\n",
      "I: 12000  - Loss: 6.088542\n",
      "I: 12100  - Loss: 6.3059897\n",
      "I: 12200  - Loss: 6.1588545\n",
      "I: 12300  - Loss: 6.466146\n",
      "I: 12400  - Loss: 5.811198\n",
      "I: 12500  - Loss: 5.8984375\n",
      "I: 12600  - Loss: 5.979167\n",
      "I: 12700  - Loss: 6.0065107\n",
      "I: 12800  - Loss: 5.916667\n",
      "I: 12900  - Loss: 6.0989585\n",
      "I: 13000  - Loss: 5.9908857\n",
      "I: 13100  - Loss: 6.015625\n",
      "I: 13200  - Loss: 5.4908857\n",
      "I: 13300  - Loss: 6.1445312\n",
      "I: 13400  - Loss: 6.359375\n",
      "I: 13500  - Loss: 6.2695312\n",
      "I: 13600  - Loss: 6.1028647\n",
      "I: 13700  - Loss: 5.8815107\n",
      "I: 13800  - Loss: 6.1132812\n",
      "I: 13900  - Loss: 5.779948\n",
      "I: 14000  - Loss: 6.0052085\n",
      "I: 14100  - Loss: 5.6992188\n",
      "I: 14200  - Loss: 5.8320312\n",
      "I: 14300  - Loss: 5.9179688\n",
      "I: 14400  - Loss: 5.7213545\n",
      "I: 14500  - Loss: 6.3151045\n",
      "I: 14600  - Loss: 5.888021\n",
      "I: 14700  - Loss: 6.1145835\n",
      "I: 14800  - Loss: 6.528646\n",
      "I: 14900  - Loss: 6.1119795\n",
      "I: 15000  - Loss: 6.4283857\n",
      "I: 15100  - Loss: 5.703125\n",
      "I: 15200  - Loss: 5.7252607\n",
      "I: 15300  - Loss: 5.6315107\n",
      "I: 15400  - Loss: 5.5364585\n",
      "I: 15500  - Loss: 5.7109375\n",
      "I: 15600  - Loss: 5.7513022\n",
      "I: 15700  - Loss: 5.6054688\n",
      "I: 15800  - Loss: 5.7890625\n",
      "I: 15900  - Loss: 6.3684897\n",
      "I: 16000  - Loss: 6.0234375\n",
      "I: 16100  - Loss: 5.764323\n",
      "I: 16200  - Loss: 5.869792\n",
      "I: 16300  - Loss: 6.654948\n",
      "I: 16400  - Loss: 6.3606772\n",
      "I: 16500  - Loss: 6.2838545\n",
      "I: 16600  - Loss: 5.8476562\n",
      "I: 16700  - Loss: 6.0351562\n",
      "I: 16800  - Loss: 5.7981772\n",
      "I: 16900  - Loss: 5.7708335\n",
      "I: 17000  - Loss: 5.8619795\n",
      "I: 17100  - Loss: 6.182292\n",
      "I: 17200  - Loss: 5.904948\n",
      "I: 17300  - Loss: 6.1744795\n",
      "I: 17400  - Loss: 6.2265625\n",
      "I: 17500  - Loss: 5.9296875\n",
      "I: 17600  - Loss: 6.1575522\n",
      "I: 17700  - Loss: 6.1119795\n",
      "I: 17800  - Loss: 5.979167\n",
      "I: 17900  - Loss: 5.936198\n",
      "I: 18000  - Loss: 6.171875\n",
      "I: 18100  - Loss: 6.3463545\n",
      "I: 18200  - Loss: 5.8007812\n",
      "I: 18300  - Loss: 5.760417\n",
      "I: 18400  - Loss: 5.8554688\n",
      "I: 18500  - Loss: 6.2083335\n",
      "I: 18600  - Loss: 6.0351562\n",
      "I: 18700  - Loss: 6.1432295\n",
      "I: 18800  - Loss: 5.9908857\n",
      "I: 18900  - Loss: 5.8398438\n",
      "I: 19000  - Loss: 6.2825522\n",
      "I: 19100  - Loss: 6.4179688\n",
      "I: 19200  - Loss: 6.5351562\n",
      "I: 19300  - Loss: 6.169271\n",
      "I: 19400  - Loss: 6.1679688\n",
      "I: 19500  - Loss: 6.154948\n",
      "I: 19600  - Loss: 5.8763022\n",
      "I: 19700  - Loss: 5.7903647\n",
      "I: 19800  - Loss: 5.7200522\n",
      "I: 19900  - Loss: 5.6575522\n",
      "I: 20000  - Loss: 5.7057295\n",
      "I: 20100  - Loss: 6.0208335\n",
      "I: 20200  - Loss: 6.2578125\n",
      "I: 20300  - Loss: 6.1132812\n",
      "I: 20400  - Loss: 6.184896\n",
      "I: 20500  - Loss: 6.2526045\n",
      "I: 20600  - Loss: 5.7981772\n",
      "I: 20700  - Loss: 6.1835938\n",
      "I: 20800  - Loss: 6.3802085\n",
      "I: 20900  - Loss: 5.8867188\n",
      "I: 21000  - Loss: 5.9114585\n",
      "I: 21100  - Loss: 5.717448\n",
      "I: 21200  - Loss: 5.7421875\n",
      "I: 21300  - Loss: 5.6106772\n",
      "I: 21400  - Loss: 5.3828125\n",
      "I: 21500  - Loss: 5.9583335\n",
      "I: 21600  - Loss: 6.171875\n",
      "I: 21700  - Loss: 6.1432295\n",
      "I: 21800  - Loss: 6.029948\n",
      "I: 21900  - Loss: 5.936198\n",
      "I: 22000  - Loss: 5.8789062\n",
      "I: 22100  - Loss: 5.9544272\n",
      "I: 22200  - Loss: 6.8164062\n",
      "I: 22300  - Loss: 6.322917\n",
      "I: 22400  - Loss: 6.1679688\n",
      "I: 22500  - Loss: 5.4283857\n",
      "I: 22600  - Loss: 5.9114585\n",
      "I: 22700  - Loss: 5.841146\n",
      "I: 22800  - Loss: 6.3450522\n",
      "I: 22900  - Loss: 5.9765625\n",
      "I: 23000  - Loss: 5.7838545\n",
      "I: 23100  - Loss: 5.7682295\n",
      "I: 23200  - Loss: 6.0664062\n",
      "I: 23300  - Loss: 6.0807295\n",
      "I: 23400  - Loss: 6.092448\n",
      "I: 23500  - Loss: 5.9752607\n",
      "I: 23600  - Loss: 6.0052085\n",
      "I: 23700  - Loss: 6.1796875\n",
      "I: 23800  - Loss: 6.1315107\n",
      "I: 23900  - Loss: 6.203125\n",
      "I: 24000  - Loss: 5.703125\n",
      "I: 24100  - Loss: 6.0039062\n",
      "I: 24200  - Loss: 5.8997397\n",
      "I: 24300  - Loss: 5.7903647\n",
      "I: 24400  - Loss: 6.0091147\n",
      "I: 24500  - Loss: 5.934896\n",
      "I: 24600  - Loss: 5.760417\n",
      "I: 24700  - Loss: 5.7213545\n",
      "I: 24800  - Loss: 6.2851562\n",
      "I: 24900  - Loss: 6.2682295\n",
      "I: 25000  - Loss: 5.5169272\n",
      "I: 25100  - Loss: 6.1054688\n",
      "I: 25200  - Loss: 6.1171875\n",
      "I: 25300  - Loss: 5.934896\n",
      "I: 25400  - Loss: 6.044271\n",
      "I: 25500  - Loss: 6.2395835\n",
      "I: 25600  - Loss: 5.701823\n",
      "I: 25700  - Loss: 5.9921875\n",
      "I: 25800  - Loss: 5.6289062\n",
      "I: 25900  - Loss: 6.075521\n",
      "I: 26000  - Loss: 5.7747397\n",
      "I: 26100  - Loss: 6.154948\n",
      "I: 26200  - Loss: 5.760417\n",
      "I: 26300  - Loss: 6.028646\n",
      "I: 26400  - Loss: 5.873698\n",
      "I: 26500  - Loss: 6.091146\n",
      "I: 26600  - Loss: 6.21875\n",
      "I: 26700  - Loss: 6.0065107\n",
      "I: 26800  - Loss: 6.013021\n",
      "I: 26900  - Loss: 5.8658857\n",
      "I: 27000  - Loss: 5.6054688\n",
      "I: 27100  - Loss: 5.421875\n",
      "I: 27200  - Loss: 5.640625\n",
      "I: 27300  - Loss: 5.822917\n",
      "I: 27400  - Loss: 5.703125\n",
      "I: 27500  - Loss: 5.8867188\n",
      "I: 27600  - Loss: 5.7083335\n",
      "I: 27700  - Loss: 6.1992188\n",
      "I: 27800  - Loss: 5.9622397\n",
      "I: 27900  - Loss: 6.5364585\n",
      "I: 28000  - Loss: 5.796875\n",
      "I: 28100  - Loss: 6.278646\n",
      "I: 28200  - Loss: 5.890625\n",
      "I: 28300  - Loss: 5.6184897\n",
      "I: 28400  - Loss: 5.8203125\n",
      "I: 28500  - Loss: 5.5364585\n",
      "I: 28600  - Loss: 5.576823\n",
      "I: 28700  - Loss: 5.7226562\n",
      "I: 28800  - Loss: 5.8151045\n",
      "I: 28900  - Loss: 6.1289062\n",
      "I: 29000  - Loss: 6.0052085\n",
      "I: 29100  - Loss: 6.029948\n",
      "I: 29200  - Loss: 6.0169272\n",
      "I: 29300  - Loss: 5.8776045\n",
      "I: 29400  - Loss: 5.873698\n",
      "I: 29500  - Loss: 5.6015625\n",
      "I: 29600  - Loss: 5.932292\n",
      "I: 29700  - Loss: 5.6914062\n",
      "I: 29800  - Loss: 5.6927085\n",
      "I: 29900  - Loss: 5.7434897\n",
      "I: 30000  - Loss: 5.9401045\n",
      "I: 30100  - Loss: 5.6484375\n",
      "I: 30200  - Loss: 5.5338545\n",
      "I: 30300  - Loss: 5.903646\n",
      "I: 30400  - Loss: 5.7526045\n",
      "I: 30500  - Loss: 6.0234375\n",
      "I: 30600  - Loss: 5.921875\n",
      "I: 30700  - Loss: 5.6302085\n",
      "I: 30800  - Loss: 5.8645835\n",
      "I: 30900  - Loss: 6.0677085\n",
      "I: 31000  - Loss: 5.8463545\n",
      "I: 31100  - Loss: 5.748698\n",
      "I: 31200  - Loss: 5.8515625\n",
      "I: 31300  - Loss: 5.7265625\n",
      "I: 31400  - Loss: 5.5065107\n",
      "I: 31500  - Loss: 5.4570312\n",
      "I: 31600  - Loss: 6.0846357\n",
      "I: 31700  - Loss: 5.8515625\n",
      "I: 31800  - Loss: 5.9544272\n",
      "I: 31900  - Loss: 6.0338545\n",
      "I: 32000  - Loss: 6.2773438\n",
      "I: 32100  - Loss: 5.981771\n",
      "I: 32200  - Loss: 5.6210938\n",
      "I: 32300  - Loss: 6.0117188\n",
      "I: 32400  - Loss: 6.1679688\n",
      "I: 32500  - Loss: 6.4492188\n",
      "I: 32600  - Loss: 6.0833335\n",
      "I: 32700  - Loss: 6.2096357\n",
      "I: 32800  - Loss: 5.4388022\n",
      "I: 32900  - Loss: 5.4934897\n",
      "I: 33000  - Loss: 5.6835938\n",
      "I: 33100  - Loss: 5.7929688\n",
      "I: 33200  - Loss: 5.795573\n",
      "I: 33300  - Loss: 5.8710938\n",
      "I: 33400  - Loss: 5.822917\n",
      "I: 33500  - Loss: 6.139323\n",
      "I: 33600  - Loss: 5.78125\n",
      "I: 33700  - Loss: 6.029948\n",
      "I: 33800  - Loss: 6.34375\n",
      "I: 33900  - Loss: 6.4739585\n",
      "I: 34000  - Loss: 6.5703125\n",
      "I: 34100  - Loss: 5.9140625\n",
      "I: 34200  - Loss: 5.359375\n",
      "I: 34300  - Loss: 5.6992188\n",
      "I: 34400  - Loss: 5.514323\n",
      "I: 34500  - Loss: 5.6888022\n",
      "I: 34600  - Loss: 5.7981772\n",
      "I: 34700  - Loss: 5.5377607\n",
      "I: 34800  - Loss: 6.5052085\n",
      "I: 34900  - Loss: 6.1901045\n",
      "I: 35000  - Loss: 6.0494795\n",
      "I: 35100  - Loss: 5.733073\n",
      "I: 35200  - Loss: 6.078125\n",
      "I: 35300  - Loss: 6.2552085\n",
      "I: 35400  - Loss: 6.2070312\n",
      "I: 35500  - Loss: 6.244792\n",
      "I: 35600  - Loss: 6.029948\n",
      "I: 35700  - Loss: 5.828125\n",
      "I: 35800  - Loss: 5.7981772\n",
      "I: 35900  - Loss: 5.3789062\n",
      "I: 36000  - Loss: 5.854167\n",
      "I: 36100  - Loss: 5.6927085\n",
      "I: 36200  - Loss: 6.041667\n",
      "I: 36300  - Loss: 5.7513022\n",
      "I: 36400  - Loss: 6.03125\n",
      "I: 36500  - Loss: 6.260417\n",
      "I: 36600  - Loss: 5.8867188\n",
      "I: 36700  - Loss: 6.2617188\n",
      "I: 36800  - Loss: 6.557292\n",
      "I: 36900  - Loss: 6.231771\n",
      "I: 37000  - Loss: 6.1627607\n",
      "I: 37100  - Loss: 5.8606772\n",
      "I: 37200  - Loss: 5.703125\n",
      "I: 37300  - Loss: 5.684896\n",
      "I: 37400  - Loss: 5.7994795\n",
      "I: 37500  - Loss: 5.6132812\n",
      "I: 37600  - Loss: 5.9609375\n",
      "I: 37700  - Loss: 5.8867188\n",
      "I: 37800  - Loss: 5.729167\n",
      "I: 37900  - Loss: 6.0716147\n",
      "I: 38000  - Loss: 5.6002607\n",
      "I: 38100  - Loss: 5.7591147\n",
      "I: 38200  - Loss: 5.796875\n",
      "I: 38300  - Loss: 5.6627607\n",
      "I: 38400  - Loss: 6.106771\n",
      "I: 38500  - Loss: 5.5690107\n",
      "I: 38600  - Loss: 5.6171875\n",
      "I: 38700  - Loss: 5.5677085\n",
      "I: 38800  - Loss: 6.072917\n",
      "I: 38900  - Loss: 5.4882812\n",
      "I: 39000  - Loss: 5.729167\n",
      "I: 39100  - Loss: 6.420573\n",
      "I: 39200  - Loss: 5.7877607\n",
      "I: 39300  - Loss: 5.578125\n",
      "I: 39400  - Loss: 5.842448\n",
      "I: 39500  - Loss: 5.6601562\n",
      "I: 39600  - Loss: 5.9453125\n",
      "I: 39700  - Loss: 6.244792\n",
      "I: 39800  - Loss: 5.984375\n",
      "I: 39900  - Loss: 5.6966147\n",
      "I: 40000  - Loss: 5.8515625\n",
      "I: 40100  - Loss: 5.4804688\n",
      "I: 40200  - Loss: 5.5377607\n",
      "I: 40300  - Loss: 5.8463545\n",
      "I: 40400  - Loss: 5.213542\n",
      "I: 40500  - Loss: 5.8489585\n",
      "I: 40600  - Loss: 5.734375\n",
      "I: 40700  - Loss: 5.809896\n",
      "I: 40800  - Loss: 6.1888022\n",
      "I: 40900  - Loss: 5.9309897\n",
      "I: 41000  - Loss: 5.858073\n",
      "I: 41100  - Loss: 5.904948\n",
      "I: 41200  - Loss: 5.697917\n",
      "I: 41300  - Loss: 5.6953125\n",
      "I: 41400  - Loss: 5.5\n",
      "I: 41500  - Loss: 5.5976562\n",
      "I: 41600  - Loss: 5.529948\n",
      "I: 41700  - Loss: 5.559896\n",
      "I: 41800  - Loss: 6.026042\n",
      "I: 41900  - Loss: 5.6888022\n",
      "I: 42000  - Loss: 5.6627607\n",
      "I: 42100  - Loss: 6.2148438\n",
      "I: 42200  - Loss: 6.1835938\n",
      "I: 42300  - Loss: 5.967448\n",
      "I: 42400  - Loss: 5.8841147\n",
      "I: 42500  - Loss: 6.0481772\n",
      "I: 42600  - Loss: 5.885417\n",
      "I: 42700  - Loss: 5.6888022\n",
      "I: 42800  - Loss: 6.1028647\n",
      "I: 42900  - Loss: 5.9921875\n",
      "I: 43000  - Loss: 5.7890625\n",
      "I: 43100  - Loss: 5.8632812\n",
      "I: 43200  - Loss: 5.328125\n",
      "I: 43300  - Loss: 5.541667\n",
      "I: 43400  - Loss: 5.4921875\n",
      "I: 43500  - Loss: 6.1770835\n",
      "I: 43600  - Loss: 6.3658857\n",
      "I: 43700  - Loss: 5.5429688\n",
      "I: 43800  - Loss: 5.5690107\n",
      "I: 43900  - Loss: 5.859375\n",
      "I: 44000  - Loss: 5.794271\n",
      "I: 44100  - Loss: 6.25\n",
      "I: 44200  - Loss: 5.622396\n",
      "I: 44300  - Loss: 5.9440107\n",
      "I: 44400  - Loss: 5.856771\n",
      "I: 44500  - Loss: 5.666667\n",
      "I: 44600  - Loss: 5.9270835\n",
      "I: 44700  - Loss: 5.8763022\n",
      "I: 44800  - Loss: 6.0494795\n",
      "I: 44900  - Loss: 5.8372397\n",
      "I: 45000  - Loss: 5.6145835\n",
      "I: 45100  - Loss: 6.138021\n",
      "I: 45200  - Loss: 6.3802085\n",
      "I: 45300  - Loss: 5.744792\n",
      "I: 45400  - Loss: 5.903646\n",
      "I: 45500  - Loss: 6.309896\n",
      "I: 45600  - Loss: 5.7421875\n",
      "I: 45700  - Loss: 5.9570312\n",
      "I: 45800  - Loss: 6.041667\n",
      "I: 45900  - Loss: 5.640625\n",
      "I: 46000  - Loss: 6.0013022\n",
      "I: 46100  - Loss: 5.9856772\n",
      "I: 46200  - Loss: 5.4596357\n",
      "I: 46300  - Loss: 5.729167\n",
      "I: 46400  - Loss: 6.0546875\n",
      "I: 46500  - Loss: 5.8919272\n",
      "I: 46600  - Loss: 5.8007812\n",
      "I: 46700  - Loss: 5.669271\n",
      "I: 46800  - Loss: 5.8463545\n",
      "I: 46900  - Loss: 6.2539062\n",
      "I: 47000  - Loss: 6.1171875\n",
      "I: 47100  - Loss: 6.1289062\n",
      "I: 47200  - Loss: 5.7617188\n",
      "I: 47300  - Loss: 5.5963545\n",
      "I: 47400  - Loss: 6.0052085\n",
      "I: 47500  - Loss: 5.291667\n",
      "I: 47600  - Loss: 5.6002607\n",
      "I: 47700  - Loss: 5.5664062\n",
      "I: 47800  - Loss: 5.9075522\n",
      "I: 47900  - Loss: 5.8059897\n",
      "I: 48000  - Loss: 6.1341147\n",
      "I: 48100  - Loss: 5.9557295\n",
      "I: 48200  - Loss: 5.967448\n",
      "I: 48300  - Loss: 5.9739585\n",
      "I: 48400  - Loss: 5.686198\n",
      "I: 48500  - Loss: 6.09375\n",
      "I: 48600  - Loss: 6.1328125\n",
      "I: 48700  - Loss: 5.7994795\n",
      "I: 48800  - Loss: 5.901042\n",
      "I: 48900  - Loss: 6.057292\n",
      "I: 49000  - Loss: 5.8671875\n",
      "I: 49100  - Loss: 5.3632812\n",
      "I: 49200  - Loss: 5.856771\n",
      "I: 49300  - Loss: 5.9153647\n",
      "I: 49400  - Loss: 5.9570312\n",
      "I: 49500  - Loss: 5.8671875\n",
      "I: 49600  - Loss: 5.7838545\n",
      "I: 49700  - Loss: 5.934896\n",
      "I: 49800  - Loss: 6.0872397\n",
      "I: 49900  - Loss: 5.9492188\n",
      "I: 50000  - Loss: 5.9101562\n",
      "I: 50100  - Loss: 5.8515625\n",
      "I: 50200  - Loss: 5.9778647\n",
      "I: 50300  - Loss: 5.6276045\n",
      "I: 50400  - Loss: 5.9270835\n",
      "I: 50500  - Loss: 5.4921875\n",
      "I: 50600  - Loss: 5.6575522\n",
      "I: 50700  - Loss: 5.3945312\n",
      "I: 50800  - Loss: 5.807292\n",
      "I: 50900  - Loss: 6.234375\n",
      "I: 51000  - Loss: 6.1588545\n",
      "I: 51100  - Loss: 6.0364585\n",
      "I: 51200  - Loss: 6.3033857\n",
      "I: 51300  - Loss: 5.9700522\n",
      "I: 51400  - Loss: 5.8190107\n",
      "I: 51500  - Loss: 5.889323\n",
      "I: 51600  - Loss: 5.46875\n",
      "I: 51700  - Loss: 5.7408857\n",
      "I: 51800  - Loss: 5.5273438\n",
      "I: 51900  - Loss: 5.7252607\n",
      "I: 52000  - Loss: 5.8815107\n",
      "I: 52100  - Loss: 5.654948\n",
      "I: 52200  - Loss: 5.651042\n",
      "I: 52300  - Loss: 5.733073\n",
      "I: 52400  - Loss: 5.5546875\n",
      "I: 52500  - Loss: 5.9466147\n",
      "I: 52600  - Loss: 6.0481772\n",
      "I: 52700  - Loss: 6.106771\n",
      "I: 52800  - Loss: 6.390625\n",
      "I: 52900  - Loss: 6.1106772\n",
      "I: 53000  - Loss: 5.7877607\n",
      "I: 53100  - Loss: 5.7747397\n",
      "I: 53200  - Loss: 5.5091147\n",
      "I: 53300  - Loss: 5.3932295\n",
      "I: 53400  - Loss: 5.5\n",
      "I: 53500  - Loss: 5.46875\n",
      "I: 53600  - Loss: 5.6367188\n",
      "I: 53700  - Loss: 5.9270835\n",
      "I: 53800  - Loss: 5.9270835\n",
      "I: 53900  - Loss: 5.7044272\n",
      "I: 54000  - Loss: 5.936198\n",
      "I: 54100  - Loss: 6.1614585\n",
      "I: 54200  - Loss: 5.8828125\n",
      "I: 54300  - Loss: 5.889323\n",
      "I: 54400  - Loss: 5.8359375\n",
      "I: 54500  - Loss: 5.828125\n",
      "I: 54600  - Loss: 5.9934897\n",
      "I: 54700  - Loss: 5.731771\n",
      "I: 54800  - Loss: 5.325521\n",
      "I: 54900  - Loss: 5.7434897\n",
      "I: 55000  - Loss: 5.420573\n",
      "I: 55100  - Loss: 5.7994795\n",
      "I: 55200  - Loss: 5.9960938\n",
      "I: 55300  - Loss: 5.6744795\n",
      "I: 55400  - Loss: 5.578125\n",
      "I: 55500  - Loss: 5.6184897\n",
      "I: 55600  - Loss: 5.3242188\n",
      "I: 55700  - Loss: 5.8763022\n",
      "I: 55800  - Loss: 6.0585938\n",
      "I: 55900  - Loss: 6.0039062\n",
      "I: 56000  - Loss: 5.7721357\n",
      "I: 56100  - Loss: 5.717448\n",
      "I: 56200  - Loss: 5.5338545\n",
      "I: 56300  - Loss: 5.5234375\n",
      "I: 56400  - Loss: 5.8020835\n",
      "I: 56500  - Loss: 5.8307295\n",
      "I: 56600  - Loss: 6.322917\n",
      "I: 56700  - Loss: 5.7434897\n",
      "I: 56800  - Loss: 6.0507812\n",
      "I: 56900  - Loss: 5.9609375\n",
      "I: 57000  - Loss: 5.8515625\n",
      "I: 57100  - Loss: 6.119792\n",
      "I: 57200  - Loss: 6.1445312\n",
      "I: 57300  - Loss: 6.2239585\n",
      "I: 57400  - Loss: 6.0559897\n",
      "I: 57500  - Loss: 5.6575522\n",
      "I: 57600  - Loss: 5.7083335\n",
      "I: 57700  - Loss: 5.8841147\n",
      "I: 57800  - Loss: 5.5859375\n",
      "I: 57900  - Loss: 5.7851562\n",
      "I: 58000  - Loss: 6.075521\n",
      "I: 58100  - Loss: 5.7460938\n",
      "I: 58200  - Loss: 5.9713545\n",
      "I: 58300  - Loss: 6.1640625\n",
      "I: 58400  - Loss: 5.6575522\n"
     ]
    }
   ],
   "source": [
    "N = train_data.sizes['time'] - train_steps - 4\n",
    "loss_array = []\n",
    "\n",
    "for i in range(N):\n",
    "    train_batch = train_data.isel(time=slice(i, i + train_steps + 2))\n",
    "    train_batch = train_batch.compute()\n",
    "\n",
    "    train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    train_batch, target_lead_times=slice(\"6h\", f\"{train_steps*6}h\"),\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "    # calculate loss and gradients\n",
    "    loss, diagnostics, next_state, grads = grads_fn_jitted(params, state, train_inputs, train_targets, train_forcings)\n",
    "\n",
    "    # update\n",
    "    updates, opt_state = optimiser.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    loss_array.append(loss)\n",
    "    if i%100 == 0:\n",
    "        print(\"I:\", i , \" - Loss:\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334b1529acc30f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34d5dea497941b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T13:48:28.913640Z",
     "start_time": "2024-04-11T13:47:03.595074Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# @title Autoregressive rollout (loop in python)\n",
    "\n",
    "print(\"Inputs:  \", eval_inputs.dims.mapping)\n",
    "print(\"Targets: \", eval_targets.dims.mapping)\n",
    "print(\"Forcings:\", eval_forcings.dims.mapping)\n",
    "\n",
    "predictions = rollout.chunked_prediction(\n",
    "    run_forward_jitted,\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=eval_inputs,\n",
    "    targets_template=eval_targets * np.nan,\n",
    "    forcings=eval_forcings)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c556a-29a6-4712-b253-00c12b586ffc",
   "metadata": {},
   "source": [
    "# Model running loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b95758-129d-4953-8370-52fac2528480",
   "metadata": {},
   "source": [
    "N = eval_data.sizes['time'] - eval_steps - 4\n",
    "# Open a zarr store to write predictions to disk\n",
    "store = xr.backends.ZarrStore(\"../preds/predictions.zarr\", \"w\")\n",
    "\n",
    "for i in range(10):\n",
    "    eval_batch = eval_data.isel(time=slice(i, i + eval_steps + 2))\n",
    "    eval_batch = eval_batch.compute()\n",
    "\n",
    "    eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    eval_batch, target_lead_times=slice(\"6h\", f\"{eval_steps*6}h\"),\n",
    "    **dataclasses.asdict(task_config))\n",
    "\n",
    "    predictions = rollout.chunked_prediction(\n",
    "        run_forward_jitted,\n",
    "        rng=jax.random.PRNGKey(0),\n",
    "        inputs=eval_inputs,\n",
    "        targets_template=eval_targets * np.nan,\n",
    "        forcings=eval_forcings)\n",
    "    # Write the prediction dataset to the zarr store\n",
    "    predictions.to_zarr(store, group=f\"prediction_time_step_{prediction_time_step}\", mode=\"a\")\n",
    "\n",
    "# Close the zarr store\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce3bcc-c48e-411e-a7f3-2b881ac925e0",
   "metadata": {},
   "source": [
    "# Read the predictions back into memory and concatenate them along the 'pred_times' dimension\n",
    "predictions = xr.open_zarr(\"../preds/predictions.zarr\", concat_dim=\"prediction_time_step\")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481c19e5f3d2785",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Save the evaluation run to file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42905d64-749f-472c-8a2c-4f1cca97ac0a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Specify the path where you want to save the Zarr file\n",
    "zarr_path = \"Eval_preds/mse_64x32_2020.zarr\"\n",
    "\n",
    "# Save the dataset to the Zarr file\n",
    "predictions.to_zarr(zarr_path, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a73a19c6364468",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Save the model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7382ba7b-b0b1-4e43-80d1-15537cabc139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import os \n",
    "\n",
    "def flatten_dict(d, parent_key='', sep='//'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def save_model_params(d, file_path):\n",
    "    flat_dict = flatten_dict(d)\n",
    "    # Convert JAX arrays to NumPy for saving\n",
    "    np_dict = {k: np.array(v) if isinstance(v, jnp.ndarray) else v for k, v in flat_dict.items()}\n",
    "    np.savez(file_path, **np_dict)\n",
    "\n",
    "params_path = os.path.join('../models', 'params_64x32_mae.npz')\n",
    "save_model_params(params, params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c5fb7cab06b29",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now, our trained model is saved to a file ,which can be used to load and run again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MTLMenv",
   "language": "python",
   "name": "mtlmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
